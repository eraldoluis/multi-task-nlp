"""Main module.

This code was initially generated by Claude and subsequently modified by Eraldo R. Fernandes.

The following prompt was used:
> Create a code to train an intent classifier using PyTorch Lightning. The code has to include validation and test
> procedures. It should use the HuggingFace dataset "tuetschek/atis" which comprises two splits: train and test. The
> train split should be divided into train and validation. This dataset includes three columns: "intent", "text" and
> "slots". You must ignore the "slots" column and use the "intent" column as the label and the "text" column as the
> input for the model. The model should be based on the "distilbert/distilbert-base-uncased" model from HuggingFace. A
> classification layer should be added on top of this model to predict the intent. The input for the classification
> layer should be the hidden representation of the first token of the input.
"""

import numpy as np
import pytorch_lightning as pl
import torch
import torch.nn as nn
from datasets import load_dataset
from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint
from sklearn.preprocessing import LabelEncoder
from torch.utils.data import DataLoader, Dataset
from transformers import AutoModel, AutoTokenizer
import wandb


class IntentDataset(Dataset):
    """Dataset class for intent classification."""

    def __init__(self, texts: list[str], labels: list[int], tokenizer, max_length: int = 128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]

        encoding = self.tokenizer(
            text,
            add_special_tokens=True,
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt",
        )

        return {
            "input_ids": encoding["input_ids"].flatten(),
            "attention_mask": encoding["attention_mask"].flatten(),
            "labels": torch.tensor(label, dtype=torch.long),
        }


class IntentClassifier(nn.Module):
    """Intent classification model using DistilBERT."""

    def __init__(self, model_name: str, num_classes: int, dropout: float = 0.3):
        super().__init__()
        self.bert = AutoModel.from_pretrained(model_name)
        self.dropout = nn.Dropout(dropout)
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        # Use the hidden state of the first token ([CLS])
        cls_output = outputs.last_hidden_state[:, 0, :]
        cls_output = self.dropout(cls_output)
        logits = self.classifier(cls_output)
        return logits


class IntentClassifierLightning(pl.LightningModule):
    """PyTorch Lightning module for intent classification."""

    def __init__(self, model_name: str, num_classes: int, learning_rate: float = 2e-5):
        super().__init__()
        self.save_hyperparameters()
        self.model = IntentClassifier(model_name, num_classes)
        self.criterion = nn.CrossEntropyLoss()
        self.learning_rate = learning_rate

        # Initialize Weights and Biases
        wandb.init(project='your_project_name', entity='your_entity_name')
        self.log_hyperparams()  # Log hyperparameters to W&B

    def log_hyperparams(self):
        wandb.config.update({
            "model_name": self.hparams.model_name,
            "num_classes": self.hparams.num_classes,
            "learning_rate": self.hparams.learning_rate,
        })

    def forward(self, input_ids, attention_mask):
        return self.model(input_ids, attention_mask)

    def training_step(self, batch, batch_idx):
        input_ids = batch["input_ids"]
        attention_mask = batch["attention_mask"]
        labels = batch["labels"]

        logits = self(input_ids, attention_mask)
        loss = self.criterion(logits, labels)

        preds = torch.argmax(logits, dim=1)
        acc = (preds == labels).float().mean()

        self.log("train_loss", loss, prog_bar=True)
        self.log("train_acc", acc, prog_bar=True)

        # Log metrics to W&B
        wandb.log({"train_loss": loss, "train_acc": acc})

        return loss

    def validation_step(self, batch, batch_idx):
        input_ids = batch["input_ids"]
        attention_mask = batch["attention_mask"]
        labels = batch["labels"]

        logits = self(input_ids, attention_mask)
        loss = self.criterion(logits, labels)

        preds = torch.argmax(logits, dim=1)
        acc = (preds == labels).float().mean()

        self.log("val_loss", loss, prog_bar=True)
        self.log("val_acc", acc, prog_bar=True)

        # Log metrics to W&B
        wandb.log({"val_loss": loss, "val_acc": acc})

        return loss

    def test_step(self, batch, batch_idx):
        input_ids = batch["input_ids"]
        attention_mask = batch["attention_mask"]
        labels = batch["labels"]

        logits = self(input_ids, attention_mask)
        loss = self.criterion(logits, labels)

        preds = torch.argmax(logits, dim=1)
        acc = (preds == labels).float().mean()

        self.log("test_loss", loss, prog_bar=True)
        self.log("test_acc", acc, prog_bar=True)

        # Log metrics to W&B
        wandb.log({"test_loss": loss, "test_acc": acc})

        return {"test_loss": loss, "test_acc": acc}

    def configure_optimizers(self):
        # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        #     optimizer, mode="min", factor=0.5, patience=2, verbose=True
        # )
        # return {"optimizer": optimizer, "lr_scheduler": {"scheduler": scheduler, "monitor": "val_loss"}}
        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate)


class IntentDataModule(pl.LightningDataModule):
    """Data module for loading and preparing the ATIS dataset."""

    def __init__(self, model_name: str, batch_size: int = 32, max_length: int = 128, val_split: float = 0.15):
        super().__init__()
        self.model_name = model_name
        self.batch_size = batch_size
        self.max_length = max_length
        self.val_split = val_split
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.label_encoder = LabelEncoder()

    def prepare_data(self):
        # Download dataset
        load_dataset("tuetschek/atis")

    def setup(self, stage=None):
        # Load dataset
        dataset = load_dataset("tuetschek/atis")

        # Extract train and test data
        train_data = dataset["train"]
        test_data = dataset["test"]

        # Fit label encoder on all labels
        all_labels = list(train_data["intent"]) + list(test_data["intent"])
        self.label_encoder.fit(all_labels)
        self.num_classes = len(self.label_encoder.classes_)

        # Split train into train and validation
        train_size = int((1 - self.val_split) * len(train_data))
        indices = np.random.permutation(len(train_data))
        train_indices = indices[:train_size]
        val_indices = indices[train_size:]

        # Create train dataset
        train_texts = [train_data["text"][i] for i in train_indices]
        train_labels = self.label_encoder.transform([train_data["intent"][i] for i in train_indices])
        self.train_dataset = IntentDataset(train_texts, train_labels, self.tokenizer, self.max_length)

        # Create validation dataset
        val_texts = [train_data["text"][i] for i in val_indices]
        val_labels = self.label_encoder.transform([train_data["intent"][i] for i in val_indices])
        self.val_dataset = IntentDataset(val_texts, val_labels, self.tokenizer, self.max_length)

        # Create test dataset
        test_texts = test_data["text"]
        test_labels = self.label_encoder.transform(test_data["intent"])
        self.test_dataset = IntentDataset(test_texts, test_labels, self.tokenizer, self.max_length)

        print(f"Number of classes: {self.num_classes}")
        print(f"Train size: {len(self.train_dataset)}")
        print(f"Validation size: {len(self.val_dataset)}")
        print(f"Test size: {len(self.test_dataset)}")

    def train_dataloader(self):
        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=4)

    def val_dataloader(self):
        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=4)

    def test_dataloader(self):
        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=4)


def train_intent_classifier():
    """Main training function."""

    # Set random seed for reproducibility
    pl.seed_everything(42)

    # Parameters
    MODEL_NAME = "distilbert/distilbert-base-uncased"
    BATCH_SIZE = 32
    MAX_EPOCHS = 10
    LEARNING_RATE = 2e-5

    # Initialize data module
    data_module = IntentDataModule(model_name=MODEL_NAME, batch_size=BATCH_SIZE, max_length=128, val_split=0.15)
    data_module.setup()

    # Initialize model
    model = IntentClassifierLightning(
        model_name=MODEL_NAME, num_classes=data_module.num_classes, learning_rate=LEARNING_RATE
    )

    # Callbacks
    checkpoint_callback = ModelCheckpoint(
        monitor="val_loss",
        dirpath="checkpoints",
        filename="intent-classifier-{epoch:02d}-{val_loss:.2f}",
        save_top_k=3,
        mode="min",
    )

    early_stop_callback = EarlyStopping(monitor="val_loss", patience=3, mode="min", verbose=True)

    # Initialize trainer
    trainer = pl.Trainer(
        max_epochs=MAX_EPOCHS,
        callbacks=[checkpoint_callback, early_stop_callback],
        accelerator="auto",
        devices=1,
        log_every_n_steps=10,
    )

    # Train model
    print("Starting training...")
    trainer.fit(model, data_module)

    # Test model
    print("\nTesting model on test set...")
    trainer.test(model, data_module)

    print("\nTraining complete!")
    print(f"Best model saved at: {checkpoint_callback.best_model_path}")


if __name__ == "__main__":
    train_intent_classifier()
