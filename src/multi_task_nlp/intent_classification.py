"""Main module.

This code was initially generated by Claude and subsequently modified by Eraldo R. Fernandes.

The following prompt was used:
> Create a code to train an intent classifier using PyTorch Lightning. The code has to include validation and test
> procedures. It should use the HuggingFace dataset "tuetschek/atis" which comprises two splits: train and test. The
> train split should be divided into train and validation. This dataset includes three columns: "intent", "text" and
> "slots". You must ignore the "slots" column and use the "intent" column as the label and the "text" column as the
> input for the model. The model should be based on the "distilbert/distilbert-base-uncased" model from HuggingFace. A
> classification layer should be added on top of this model to predict the intent. The input for the classification
> layer should be the hidden representation of the first token of the input.
"""

import numpy as np
import pytorch_lightning as pl
import torch
import torch.nn as nn
from datasets import load_dataset
from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint
from pytorch_lightning.loggers import WandbLogger
from torch.utils.data import DataLoader, Dataset
from transformers import AutoModel, AutoTokenizer

import wandb
from multi_task_nlp.util import LabelEncoder


class TextClassificationDataset(Dataset):
    """Dataset class for text classification."""

    # FIXME (eraldoluis): this implementation is quite inefficient mainly due to two things:
    # - It tokenizes one input at a time, not making use of FastTokenizers parallelization (batch).
    # - It pads all sequences to max_length, instead of using dynamic padding per batch.
    #
    # I also don't like converting the original dataset to lists of texts and labels just to create another dataset.
    # Instead we could use the map method from the datasets library to tokenize the inputs and create a new dataset.

    def __init__(self, texts: list[str], labels: list[int], tokenizer, max_length: int = 128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]

        encoding = self.tokenizer(
            text,
            add_special_tokens=True,
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt",
        )

        return {
            "input_ids": encoding["input_ids"].flatten(),
            "attention_mask": encoding["attention_mask"].flatten(),
            "labels": torch.tensor(label, dtype=torch.long),
        }


class TextClassificationDataModule(pl.LightningDataModule):
    """Data module for loading and preparing the ATIS dataset."""

    INTENTS = [
        "flight",
        "flight_time",
        "airfare",
        "aircraft",
        "ground_service",
        "airport",
        "airline",
        "distance",
        "abbreviation",
        "ground_fare",
        "quantity",
        "city",
        "flight_no",
        "capacity",
        "meal",
        "restriction",
        "cheapest",
        "day_name",
    ]

    def __init__(self, model_name: str, batch_size: int = 32, max_length: int = 128, val_split: float = 0.15):
        super().__init__()
        self.model_name = model_name
        self.batch_size = batch_size
        self.max_length = max_length
        self.val_split = val_split
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.label_encoder = LabelEncoder(TextClassificationDataModule.INTENTS)

    @property
    def num_classes(self) -> int:
        return len(self.label_encoder.labels)

    def prepare_data(self):
        # Download dataset
        load_dataset("tuetschek/atis")

    def setup(self, stage=None):
        # Load dataset
        dataset = load_dataset("tuetschek/atis")

        # Extract train and test data
        train_data = dataset["train"]
        test_data = dataset["test"]

        # Handle multi-label intents by taking only the last intent
        dataset.map(lambda ex: {**ex, "intent": ex["intent"].split("+")[-1]}, num_proc=10)

        # Split train into train and validation
        train_size = int((1 - self.val_split) * len(train_data))
        indices = np.random.permutation(len(train_data))
        # train_indices = indices[:int(train_size*0.1)]
        train_indices = indices[:train_size]
        val_indices = indices[train_size:]

        # Create train dataset
        train_texts = [train_data["text"][i] for i in train_indices]
        train_labels = self.label_encoder.transform([train_data["intent"][i] for i in train_indices])
        self.train_dataset = TextClassificationDataset(train_texts, train_labels, self.tokenizer, self.max_length)

        # Create validation dataset
        val_texts = [train_data["text"][i] for i in val_indices]
        val_labels = self.label_encoder.transform([train_data["intent"][i] for i in val_indices])
        self.val_dataset = TextClassificationDataset(val_texts, val_labels, self.tokenizer, self.max_length)

        # Create test dataset
        test_texts = test_data["text"]
        test_labels = self.label_encoder.transform(test_data["intent"])
        self.test_dataset = TextClassificationDataset(test_texts, test_labels, self.tokenizer, self.max_length)

        print(f"Number of classes: {self.num_classes}")
        print(f"Train size: {len(self.train_dataset)}")
        print(f"Validation size: {len(self.val_dataset)}")
        print(f"Test size: {len(self.test_dataset)}")

    def train_dataloader(self):
        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=4)

    def val_dataloader(self):
        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=4)

    def test_dataloader(self):
        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=4)


class TextClassifier(nn.Module):
    """Intent classification model using DistilBERT."""

    def __init__(self, model_name: str, num_classes: int, freeze_bert: bool = False):
        super().__init__()
        self.bert = AutoModel.from_pretrained(model_name)

        if freeze_bert:
            # FIXME (eraldoluis): if we freeze the whole BERT encoder, the model can't learn because it is using the
            # hidden representation of the first token ([CLS]) as input for the classifier. This representation is not
            # meaningful for the classification task. One alternative is to unfree the last layer of BERT and freeze all
            # the rest. Another option is to add an attention pooling layer on top of BERT so that the hidden vectors of
            # all tokens are used to create a more meaningful representation for the classification task.

            # freeze all BERT params but the ones in the last layer
            idx_last_layer = self.bert.config.n_layers - 1
            for name, param in self.bert.named_parameters():
                if not name.startswith(f"transformer.layer.{idx_last_layer}."):
                    param.requires_grad = False

        # self.dropout = nn.Dropout(dropout)
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        # Use the hidden state of the first token ([CLS])
        cls_output = outputs.last_hidden_state[:, 0, :]
        # cls_output = self.dropout(cls_output)
        logits = self.classifier(cls_output)
        return logits


class TextClassifierLightning(pl.LightningModule):
    """PyTorch Lightning module for intent classification."""

    def __init__(self, model_name: str, num_classes: int, learning_rate: float = 2e-5, freeze_bert: bool = False):
        super().__init__()
        self.save_hyperparameters()
        self.model = TextClassifier(model_name, num_classes, freeze_bert=freeze_bert)
        self.loss_fn = nn.CrossEntropyLoss()
        self.learning_rate = learning_rate

    def forward(self, input_ids, attention_mask):
        return self.model(input_ids, attention_mask)

    def training_step(self, batch, batch_idx):
        input_ids = batch["input_ids"]
        attention_mask = batch["attention_mask"]
        labels = batch["labels"]

        logits = self(input_ids, attention_mask)
        loss = self.loss_fn(logits, labels)

        preds = torch.argmax(logits, dim=1)
        acc = (preds == labels).float().mean()

        self.log("train/loss", loss, prog_bar=True)
        self.log("train/acc", acc, prog_bar=True)

        return {"loss": loss, "acc": acc}

    def validation_step(self, batch, batch_idx):
        input_ids = batch["input_ids"]
        attention_mask = batch["attention_mask"]
        labels = batch["labels"]

        logits = self(input_ids, attention_mask)
        loss = self.loss_fn(logits, labels)

        preds = torch.argmax(logits, dim=1)
        acc = (preds == labels).float().mean()

        self.log("val/loss", loss, prog_bar=True)
        self.log("val/acc", acc, prog_bar=True)

        return {"loss": loss, "acc": acc}

    def test_step(self, batch, batch_idx):
        input_ids = batch["input_ids"]
        attention_mask = batch["attention_mask"]
        labels = batch["labels"]

        logits = self(input_ids, attention_mask)
        loss = self.loss_fn(logits, labels)

        preds = torch.argmax(logits, dim=1)
        acc = (preds == labels).float().mean()

        self.log("test/loss", loss, prog_bar=True)
        self.log("test/acc", acc, prog_bar=True)

        return {"loss": loss, "acc": acc}

    def configure_optimizers(self):
        # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        #     optimizer, mode="min", factor=0.5, patience=2, verbose=True
        # )
        # return {"optimizer": optimizer, "lr_scheduler": {"scheduler": scheduler, "monitor": "val_loss"}}
        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate)


def train_text_classifier():
    """Training function for text classification."""

    # TODO convert this to a function so that we can easily create a cyclopts CLI.

    # TODO add yaml config support so that we can have recipes for different experiments.

    # Set random seed for reproducibility
    pl.seed_everything(42)

    # Login to W&B
    wandb.login()

    # Parameters
    MODEL_NAME = "distilbert/distilbert-base-uncased"
    BATCH_SIZE = 32
    MAX_EPOCHS = 1
    MAX_STEPS = None  # 30
    LEARNING_RATE = 2e-5

    # Initialize data module
    data_module = TextClassificationDataModule(
        model_name=MODEL_NAME, batch_size=BATCH_SIZE, max_length=128, val_split=0.15
    )

    # Initialize model
    model = TextClassifierLightning(
        model_name=MODEL_NAME, num_classes=data_module.num_classes, learning_rate=LEARNING_RATE, freeze_bert=True
    )

    # Callbacks
    checkpoint_callback = ModelCheckpoint(
        monitor="val/loss",
        dirpath="checkpoints",
        filename="voize-intent-classifier-{epoch:02d}-{val_loss:.2f}",
        save_top_k=3,
        mode="min",
    )

    early_stop_callback = EarlyStopping(monitor="val/loss", patience=3, mode="min", verbose=True)

    # Initialize trainer
    trainer = pl.Trainer(
        max_epochs=MAX_EPOCHS,
        max_steps=MAX_STEPS,
        callbacks=[checkpoint_callback, early_stop_callback],
        accelerator="auto",
        # devices=1,
        log_every_n_steps=1,
        logger=WandbLogger(project="voyze-intent-classification"),
        val_check_interval=10,
    )

    # Train model
    print("Starting training...")
    trainer.fit(model, data_module)

    # Test model
    print("\nTesting model on test set...")
    trainer.test(model, data_module)

    print("\nTraining complete!")
    print(f"Best model saved at: {checkpoint_callback.best_model_path}")


if __name__ == "__main__":
    train_text_classifier()
